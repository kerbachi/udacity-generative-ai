{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07b8a2-0123-4857-a280-d01d0335e27f",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: **LoRA**\n",
    "* Model: **google-bert/bert-base-cased**\n",
    "* Evaluation approach: **Using accuracy metric**\n",
    "* Fine-tuning dataset: **stanfordnlp/imdb**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d5e9f3-c0e8-4e02-9621-d92132117997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate scikit-learn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d29dae-27b0-4ffa-b51e-baa9961dfcfc",
   "metadata": {},
   "source": [
    "## Prepare the Foundation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9f8e5-75ec-47cd-b1b5-72ed07c0ff2d",
   "metadata": {},
   "source": [
    "### Load a pretrained HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce2c060-d8a5-4698-95b6-3162c357f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_id=\"google-bert/bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd52b1d-3019-4725-b98f-5f453467bab8",
   "metadata": {},
   "source": [
    "### Load and preprocess a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train_datasets = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_test_datasets = dataset[\"test\"].map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36df5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_train_datasets.shuffle(seed=42).select(range(3000))\n",
    "small_eval_dataset = tokenized_test_datasets.shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9e9886-23e7-4742-8698-5e330adc1d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(small_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8780c749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 2\n",
      "the labels: ['neg', 'pos']\n",
      "id2label= {0: 'neg', 1: 'pos'}\n"
     ]
    }
   ],
   "source": [
    "#From: https://achimoraites.medium.com/lightweight-roberta-sequence-classification-fine-tuning-with-lora-using-the-hugging-face-peft-8dd9edf99d19\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Extract the number of classess and their names\n",
    "num_labels = dataset['train'].features['label'].num_classes\n",
    "class_names = dataset[\"train\"].features[\"label\"].names\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")\n",
    "\n",
    "# Create an id2label mapping\n",
    "# We will need this for our inference.\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "print(\"id2label=\", id2label)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d0ffeb-29cb-440e-9331-c05243756291",
   "metadata": {},
   "source": [
    "### Evaluate the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "854dd356-65b0-48e8-948b-1c09230d83d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id, \n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0c30a1f-ff16-4d72-a456-a1661f8b1a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af79e900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Fred Astaire is reteamed with Rita Hayworth one year after their big hit for Columbia, \"You'll Never Get Rich\". That was the movie which put Hayworth on the Hollywood map, yet her performance in this wan romantic musical hardly gives a suggestion why she was so suddenly popular. Down Buenos Aires way, a tyrannical hotel owner demands that his four daughters marry in order of age; one may think film takes place in the 18th century, but no, it's modern-day 1942. Astaire is an ex-hoofer-turned-gambler who goes back to dancing to earn some money, getting mixed up in impersonating a letter-writing admirer to Hayworth's stone-cold society beauty. Fred gazes at Rita with a brotherly smile, but she's so mannequin-like (lip-synching to her songs like a wide-eyed wind-up doll) that all romantic sparks quickly sputter. They do dance together quite comfortably, however, and the Jerome Kern score is unmemorable but not too bad. ** from ****,\n",
      "label:0\n"
     ]
    }
   ],
   "source": [
    "# Print a dataset sample\n",
    "import random\n",
    "\n",
    "\n",
    "# Generate a random integer within the range\n",
    "x = random.randint(0, 1000)\n",
    "\n",
    "print(\"text: {},\\nlabel:{}\".format(small_eval_dataset[\"text\"][x], small_eval_dataset[\"label\"][x]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62aa2908-c9e2-4f2a-bae7-cedd7aea53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use accuracy metric\n",
    "#Function inspired from https://huggingface.co/learn/nlp-course/en/chapter3/3#evaluation\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "495b8d33-0819-4670-aee4-f5301ccc496c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(output_dir=\"model_evaluation\")\n",
    "training_args = TrainingArguments(\n",
    "    \"evaluate_foundational_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "742624c3-05dc-468a-9e8c-fe46fde89841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/nx7dbc892dq643lz26s776xr0000gn/T/ipykernel_33474/1996802471.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecb8349e-b61c-4b2b-be95-dbc3a2bd95d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 01:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.82 s, sys: 722 ms, total: 2.55 s\n",
      "Wall time: 1min 6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.74080491065979,\n",
       " 'eval_model_preparation_time': 0.0031,\n",
       " 'eval_accuracy': 0.512,\n",
       " 'eval_runtime': 66.0,\n",
       " 'eval_samples_per_second': 15.152,\n",
       " 'eval_steps_per_second': 0.485}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "# Let's see the perfomance of the foundation model before any prior training\n",
    "trainer.evaluate(eval_dataset=small_eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11655fb5-3f9d-477f-85a2-096631c205f8",
   "metadata": {},
   "source": [
    "## **Without any fine tuning the model \"google-bert/bert-base-cased\" has an _accuracy_ of _0.488_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e894278-9998-4414-b4b9-ed306fee9ccb",
   "metadata": {},
   "source": [
    "### Saving the foundation model to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b41fd5e-b9be-4830-b2b8-d5203df2b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the foundational model to the local directory \"foundational_model/\" \n",
    "trainer.save_model(\"foundational_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff643e9c-aa22-407b-9426-be84cdbf83e3",
   "metadata": {},
   "source": [
    "Create two PEFT models to test two different lora_config values and compare the results between the two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41ac75-cb10-4fa5-b7da-18b9884c87a0",
   "metadata": {},
   "source": [
    "### PEFT model (Same foundational model for the two PEFT configuraiotns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = model_id \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    peft_model_id,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db754fc-bb10-4657-8722-281a5d9b9ff3",
   "metadata": {},
   "source": [
    "### Create a PEFT model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e093652-ddea-489c-b930-00c91b8cb9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an dictiopnary with two set of values for two training to see the impact on the performance of the model\n",
    "peft_values= {\n",
    "    \"values1\": {\n",
    "        \"r\": 16,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"values2\": {\n",
    "        \"r\": 64,\n",
    "        \"lora_alpha\": 128,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config1 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    r=peft_values[\"values1\"][\"r\"],\n",
    "    lora_alpha=peft_values[\"values1\"][\"lora_alpha\"],\n",
    "    lora_dropout=peft_values[\"values1\"][\"lora_dropout\"],\n",
    "    bias=peft_values[\"values1\"][\"bias\"],\n",
    "    target_modules=[\"query\", \"value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e13bde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 591,362 || all params: 108,903,172 || trainable%: 0.5430\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "lora_model1 = get_peft_model(model, lora_config1)\n",
    "lora_model1.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3e398-5fd0-41c1-ae5c-44f311a84d9d",
   "metadata": {},
   "source": [
    "### Train the PEFT model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b46862f9-750f-405e-af9e-728051e2b120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args_peft1 = TrainingArguments(\n",
    "    \"trainer_peft1_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4d4c908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 43:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.600658</td>\n",
       "      <td>0.718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.367216</td>\n",
       "      <td>0.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.524200</td>\n",
       "      <td>0.342370</td>\n",
       "      <td>0.849000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 16min 24s, total: 18min 10s\n",
      "Wall time: 43min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=564, training_loss=0.5063010411905059, metrics={'train_runtime': 2592.6722, 'train_samples_per_second': 3.471, 'train_steps_per_second': 0.218, 'total_flos': 2384349474816000.0, 'train_loss': 0.5063010411905059, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer1 = Trainer(\n",
    "    model=lora_model1,\n",
    "    args=training_args_peft1,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 01:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.02 s, sys: 4.13 s, total: 7.15 s\n",
      "Wall time: 1min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3423702120780945,\n",
       " 'eval_accuracy': 0.849,\n",
       " 'eval_runtime': 97.9669,\n",
       " 'eval_samples_per_second': 10.208,\n",
       " 'eval_steps_per_second': 0.643,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer1.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9af57",
   "metadata": {},
   "source": [
    "###### **With fine tuning the model1 \"google-bert/bert-base-cased\" the _accuracy_ is now _0.849_ much better than the performance of the original foundational model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b2070",
   "metadata": {},
   "source": [
    "### Save the PEFT model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model1.save_pretrained(\"trainer_peft_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34d4ab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4664\n",
      "drwxr-xr-x  12 mk  staff      384  4 Dec 13:43 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "-rw-r--r--@  1 mk  staff     5101  4 Dec 13:43 README.md\n",
      "-rw-r--r--@  1 mk  staff  2372416  4 Dec 13:43 adapter_model.safetensors\n",
      "drwxr-xr-x@  5 mk  staff      160  4 Dec 13:43 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "-rw-r--r--@  1 mk  staff      681  4 Dec 13:43 adapter_config.json\n"
     ]
    }
   ],
   "source": [
    "!ls -ltra trainer_peft_1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc0b88-6ae6-453f-b8ab-75e8d0ead40c",
   "metadata": {},
   "source": [
    "### Create PEFT model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a302c8ec-c10d-430d-89a2-7e7a917917ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config2 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    r=peft_values[\"values2\"][\"r\"],\n",
    "    lora_alpha=peft_values[\"values2\"][\"lora_alpha\"],\n",
    "    lora_dropout=peft_values[\"values2\"][\"lora_dropout\"],\n",
    "    bias=peft_values[\"values2\"][\"bias\"],\n",
    "    target_modules=[\"query\", \"value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23842433-d641-4c91-adcc-048bbb88bd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,360,834 || all params: 110,672,644 || trainable%: 2.1332\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "lora_model2 = get_peft_model(model, lora_config2)\n",
    "lora_model2.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc41b7-1d61-4d88-a2f9-ba536b3f1900",
   "metadata": {},
   "source": [
    "### Train PEFT model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab85654d-f9a8-49e7-8bd0-f402e2ab9078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args_peft2 = TrainingArguments(\n",
    "    \"trainer_peft2_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b26c5076-a39c-4dd1-b4ff-329c9d57f455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 37:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.321531</td>\n",
       "      <td>0.869000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.299680</td>\n",
       "      <td>0.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.369100</td>\n",
       "      <td>0.304322</td>\n",
       "      <td>0.877000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 10min 57s, total: 12min 33s\n",
      "Wall time: 37min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=564, training_loss=0.3582543244598605, metrics={'train_runtime': 2254.0444, 'train_samples_per_second': 3.993, 'train_steps_per_second': 0.25, 'total_flos': 2433271836672000.0, 'train_loss': 0.3582543244598605, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer2 = Trainer(\n",
    "    model=lora_model2,\n",
    "    args=training_args_peft2,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3c837e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 01:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.45 s, sys: 4.83 s, total: 8.28 s\n",
      "Wall time: 1min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3043220341205597,\n",
       " 'eval_accuracy': 0.877,\n",
       " 'eval_runtime': 74.5718,\n",
       " 'eval_samples_per_second': 13.41,\n",
       " 'eval_steps_per_second': 0.845,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf6c6f",
   "metadata": {},
   "source": [
    "**With fine tuning the model2 \"google-bert/bert-base-cased\" the _accuracy_ is now _0.877_ much better than the performance of the original foundational model and the PEFT1 model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2447c43",
   "metadata": {},
   "source": [
    "### Save the PEFT model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "280e874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model1.save_pretrained(\"trainer_peft_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01468279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 9256\r\n",
      "drwxr-xr-x 10 student student    4096 Dec  4 16:13 ..\r\n",
      "-rw-r--r--  1 student student      88 Dec  4 16:13 README.md\r\n",
      "-rw-r--r--  1 student student 9461447 Dec  4 16:13 adapter_model.bin\r\n",
      "-rw-r--r--  1 student student     449 Dec  4 16:13 adapter_config.json\r\n",
      "drwxr-xr-x  2 student student    4096 Dec  4 16:13 .\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltra trainer_peft_2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefb3fe-6110-49f4-981c-7a87cb76b2e1",
   "metadata": {},
   "source": [
    "## Perform Inference Using the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679d481",
   "metadata": {},
   "source": [
    "### Load the saved PEFT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d6d180",
   "metadata": {},
   "source": [
    "We load the best PEFT model of the two we created: \"trainer_peft_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "saved_model = AutoModelForSequenceClassification.from_pretrained(\"trainer_peft_2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c08ece",
   "metadata": {},
   "source": [
    "### Evaluate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Class: 0, Label: neg,\n",
      "Text: This is the most frightening film ever made in Hollywood. It is a cautionary tale of how to take a European masterpiece and suck the life of of it until it is a dry husk like an insect carcass on the the windowsill. Frightening because it reveals how the world of Hollywood really works: ignorant money begetting dross. It makes me wonder how many great films could populate the corridors of my memory if the Hollywood process had not leveled them to forgettable mediocrity. Cry for the murdered children! See Spoorloos or read The Golden Egg, if you dare, because they will come back to you forever in the idle moments of your life: when you're walking along the street and you see a 'missing' poster; in ordinary-looking parking lots; when you hear the Tour De France on the radio; and, especially, when you you think \"what's the harm?\" in wearing a sock with a hole in it on a perfectly ordinary day.<br /><br />If only I could give this a zero.\n",
      "\n",
      "From the dataset, the text is classified as: 0: neg\n",
      "\n",
      "CPU times: user 5.22 s, sys: 28.4 ms, total: 5.25 s\n",
      "Wall time: 5.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# classify function from URL: https://achimoraites.medium.com/lightweight-roberta-sequence-classification-fine-tuning-with-lora-using-the-hugging-face-peft-8dd9edf99d19\n",
    "\n",
    "x = random.randint(0, 1000)\n",
    "\n",
    "text_to_classify=small_eval_dataset[\"text\"][x]\n",
    "\n",
    "def classify(text):\n",
    "  inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "  output = saved_model(**inputs)\n",
    "\n",
    "  prediction = output.logits.argmax(dim=-1).item()\n",
    "\n",
    "  print(f'\\n Class: {prediction}, Label: {id2label[prediction]},\\nText: {text}')\n",
    "\n",
    "\n",
    "\n",
    "classify(text_to_classify)\n",
    "\n",
    "print(\"\\nFrom the dataset, the text is classified as: {}: {}\\n\".format(small_eval_dataset[\"label\"][x], id2label[small_eval_dataset[\"label\"][x]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03bc9e",
   "metadata": {},
   "source": [
    "**The inference classified the text as negative which matches what the datatsets has as label.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
