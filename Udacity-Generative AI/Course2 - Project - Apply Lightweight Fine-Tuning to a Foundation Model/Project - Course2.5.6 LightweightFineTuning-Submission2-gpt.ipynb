{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07b8a2-0123-4857-a280-d01d0335e27f",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: **LoRA**\n",
    "* Model: **google-bert/bert-base-cased**\n",
    "* Evaluation approach: **Using accuracy metric**\n",
    "* Fine-tuning dataset: **stanfordnlp/imdb**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f3aade4-94e1-4d5a-8730-79ede2b53963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -U accelerate -q\n",
    "! pip install -U transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d5e9f3-c0e8-4e02-9621-d92132117997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12ab3b92-b495-4fd9-9f54-d1a94ea16f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02d3e4c5-cd79-41ba-9fa1-e21f49874aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d29dae-27b0-4ffa-b51e-baa9961dfcfc",
   "metadata": {},
   "source": [
    "## Prepare the Foundation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9f8e5-75ec-47cd-b1b5-72ed07c0ff2d",
   "metadata": {},
   "source": [
    "### Load a pretrained HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ce2c060-d8a5-4698-95b6-3162c357f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2Tokenizer\n",
    "model_id=\"openai-community/gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id, pad_token = '[PAD]')\n",
    "tokenizer.padding_side = \"right\" \n",
    "# tokenizer.pad_token = '[PAD]' #tokenizer.eos_token # to avoid an error\n",
    "# tokenizer.pad_token = GPT2Tokenizer.eos_token # to avoid an error\n",
    "\n",
    "# GPT2_tokenizer.pad_token = GPT2_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c77aee-0d88-4a7d-bb34-ca916fb51aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd52b1d-3019-4725-b98f-5f453467bab8",
   "metadata": {},
   "source": [
    "### Load and preprocess a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf54e098c3c426dbce64200ce0e2b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ae77b7da794daa833117cbc1be9b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\") #no padding for GPT\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors='pt') #no padding for GPT\n",
    "\n",
    "tokenized_train_datasets = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_test_datasets = dataset[\"test\"].map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36df5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_train_datasets.shuffle(seed=42).select(range(3000))\n",
    "small_eval_dataset = tokenized_test_datasets.shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9e9886-23e7-4742-8698-5e330adc1d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(small_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8780c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From: https://achimoraites.medium.com/lightweight-roberta-sequence-classification-fine-tuning-with-lora-using-the-hugging-face-peft-8dd9edf99d19\n",
    "\n",
    "# from transformers import DataCollatorWithPadding\n",
    "\n",
    "# # Extract the number of classess and their names\n",
    "# num_labels = dataset['train'].features['label'].num_classes\n",
    "# class_names = dataset[\"train\"].features[\"label\"].names\n",
    "# print(f\"number of labels: {num_labels}\")\n",
    "# print(f\"the labels: {class_names}\")\n",
    "\n",
    "# # Create an id2label mapping\n",
    "# # We will need this for our inference.\n",
    "# id2label = {i: label for i, label in enumerate(class_names)}\n",
    "# print(\"id2label=\", id2label)\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d0ffeb-29cb-440e-9331-c05243756291",
   "metadata": {},
   "source": [
    "### Evaluate the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab234547-ea90-400b-ae12-fbb2c0c253ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\n",
      "Requirement already satisfied: torch in /Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages (2.6.0.dev20241221)\n",
      "Requirement already satisfied: torchvision in /Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages (0.22.0.dev20241221)\n",
      "Requirement already satisfied: torchaudio in /Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages (2.6.0.dev20241221)\n",
      "Requirement already satisfied: filelock in /Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages (from torchvision) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mk/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch\n",
    "!pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "854dd356-65b0-48e8-948b-1c09230d83d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2ForSequenceClassification #AutoModelForSequenceClassification\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    model_id, \n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0c30a1f-ff16-4d72-a456-a1661f8b1a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af79e900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Gritty drama? Emotionally powerful? Blah! The BBC has lost out big time to masterful productions such as The Wire, Sopranos and Carnivale from HBO. Okay, so the budget may be a lot smaller but 'The Street' last night was badly acted, predictable, unrealistic, stereotypical, insensitive and a big fat waste of time. TV (British TV) is not as good as it used to be and is falling further and further behind the American productions.<br /><br />There was no sense of brutal violence from the 'local gangster'. There was no indication that this man was 'insane' enough to beat up a man he has respected for such a long time. There was no remorse when he did it and this shouldn't be the sort of character that would back down when Bob Hoskins called his son a pansy, in a display of 'bravery'.<br /><br />I wish I was more eloquent to express my disdain for this show, but I am not and although I can't prove my point well enough, believe me when I say that this was rubbish, shock TV, that provides no real inward looking perspective on life.<br /><br />1/5 stars.,\n",
      "label:0\n"
     ]
    }
   ],
   "source": [
    "# Print a dataset sample\n",
    "import random\n",
    "\n",
    "\n",
    "# Generate a random integer within the range\n",
    "x = random.randint(0, 1000)\n",
    "\n",
    "print(\"text: {},\\nlabel:{}\".format(small_eval_dataset[\"text\"][x], small_eval_dataset[\"label\"][x]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62aa2908-c9e2-4f2a-bae7-cedd7aea53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use accuracy metric\n",
    "#Function inspired from https://huggingface.co/learn/nlp-course/en/chapter3/3#evaluation\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c17ad2a-6008-4d78-b93b-7e7c4f92d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- ------------------\n",
      "accelerate                1.2.1\n",
      "aiohappyeyeballs          2.4.4\n",
      "aiohttp                   3.11.11\n",
      "aiosignal                 1.3.2\n",
      "anyio                     4.7.0\n",
      "appnope                   0.1.4\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.4\n",
      "attrs                     24.3.0\n",
      "babel                     2.16.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.2.0\n",
      "certifi                   2024.12.14\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.0\n",
      "comm                      0.2.2\n",
      "datasets                  3.2.0\n",
      "debugpy                   1.8.11\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "dill                      0.3.8\n",
      "evaluate                  0.4.3\n",
      "executing                 2.1.0\n",
      "fastjsonschema            2.21.1\n",
      "filelock                  3.16.1\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.5.0\n",
      "fsspec                    2024.9.0\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.7\n",
      "httpx                     0.28.1\n",
      "huggingface-hub           0.27.0\n",
      "idna                      3.10\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.30.0\n",
      "ipywidgets                8.1.5\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.4\n",
      "joblib                    1.4.2\n",
      "json5                     0.10.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2024.10.1\n",
      "jupyter                   1.1.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.10.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.14.2\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.3.3\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "jupyterlab_widgets        3.0.13\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.0.2\n",
      "mpmath                    1.3.0\n",
      "multidict                 6.1.0\n",
      "multiprocess              0.70.16\n",
      "nbclient                  0.10.1\n",
      "nbconvert                 7.16.4\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.4.2\n",
      "notebook                  7.3.1\n",
      "notebook_shim             0.2.4\n",
      "numpy                     2.2.0\n",
      "overrides                 7.7.0\n",
      "packaging                 24.2\n",
      "pandas                    2.2.3\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pexpect                   4.9.0\n",
      "pillow                    11.0.0\n",
      "pip                       24.2\n",
      "platformdirs              4.3.6\n",
      "prometheus_client         0.21.1\n",
      "prompt_toolkit            3.0.48\n",
      "propcache                 0.2.1\n",
      "psutil                    6.1.0\n",
      "ptyprocess                0.7.0\n",
      "pure_eval                 0.2.3\n",
      "pyarrow                   18.1.0\n",
      "pycparser                 2.22\n",
      "Pygments                  2.18.0\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        3.2.1\n",
      "pytz                      2024.2\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.2.0\n",
      "referencing               0.35.1\n",
      "regex                     2024.11.6\n",
      "requests                  2.32.3\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.22.3\n",
      "safetensors               0.4.5\n",
      "scikit-learn              1.6.0\n",
      "scipy                     1.14.1\n",
      "Send2Trash                1.8.3\n",
      "setuptools                75.6.0\n",
      "six                       1.17.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.6\n",
      "stack-data                0.6.3\n",
      "sympy                     1.13.1\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.5.0\n",
      "tinycss2                  1.4.0\n",
      "tokenizers                0.21.0\n",
      "torch                     2.6.0.dev20241221\n",
      "torchaudio                2.6.0.dev20241221\n",
      "torchvision               0.22.0.dev20241221\n",
      "tornado                   6.4.2\n",
      "tqdm                      4.67.1\n",
      "traitlets                 5.14.3\n",
      "transformers              4.47.1\n",
      "types-python-dateutil     2.9.0.20241206\n",
      "typing_extensions         4.12.2\n",
      "tzdata                    2024.2\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.3\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "widgetsnbextension        4.0.13\n",
      "xxhash                    3.5.0\n",
      "yarl                      1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "495b8d33-0819-4670-aee4-f5301ccc496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(output_dir=\"model_evaluation\")\n",
    "args = TrainingArguments(\n",
    "    \"evaluate_foundational_model_gpt2\") \n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    # per_device_train_batch_size=32,\n",
    "    # per_device_eval_batch_size=32)\n",
    "\n",
    "training_args = args.set_dataloader(train_batch_size=16, eval_batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "742624c3-05dc-468a-9e8c-fe46fde89841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/nx7dbc892dq643lz26s776xr0000gn/T/ipykernel_4041/1996802471.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecb8349e-b61c-4b2b-be95-dbc3a2bd95d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Cannot handle batch sizes > 1 if no padding token is defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:4\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages/transformers/trainer.py:4050\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4047\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4049\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4050\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4051\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4053\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4054\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4060\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages/transformers/trainer.py:4244\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4241\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   4243\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 4244\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4245\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4246\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4248\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages/transformers/trainer.py:4460\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   4459\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4460\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   4461\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   4463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages/transformers/trainer.py:3708\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3706\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3707\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3708\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3709\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3710\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Training/Udacity - Generative AI/.venv/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:1608\u001b[0m, in \u001b[0;36mGPT2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1605\u001b[0m     batch_size, sequence_length \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m-> 1608\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1609\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle batch sizes > 1 if no padding token is defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1611\u001b[0m     sequence_lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Cannot handle batch sizes > 1 if no padding token is defined."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "# Let's see the perfomance of the foundation model before any prior training\n",
    "trainer.evaluate(eval_dataset=small_eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11655fb5-3f9d-477f-85a2-096631c205f8",
   "metadata": {},
   "source": [
    "## **Without any fine tuning the model \"google-bert/bert-base-cased\" has an _accuracy_ of _0.488_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e894278-9998-4414-b4b9-ed306fee9ccb",
   "metadata": {},
   "source": [
    "### Saving the foundation model to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b41fd5e-b9be-4830-b2b8-d5203df2b89e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the foundational model to the local directory \"foundational_model/\" \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfoundational_model/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the foundational model to the local directory \"foundational_model/\" \n",
    "trainer.save_model(\"foundational_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff643e9c-aa22-407b-9426-be84cdbf83e3",
   "metadata": {},
   "source": [
    "Create two PEFT models to test two different lora_config values and compare the results between the two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41ac75-cb10-4fa5-b7da-18b9884c87a0",
   "metadata": {},
   "source": [
    "### PEFT model (Same foundational model for the two PEFT configuraiotns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = model_id \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    peft_model_id,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db754fc-bb10-4657-8722-281a5d9b9ff3",
   "metadata": {},
   "source": [
    "### Create a PEFT model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e093652-ddea-489c-b930-00c91b8cb9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an dictiopnary with two set of values for two training to see the impact on the performance of the model\n",
    "peft_values= {\n",
    "    \"values1\": {\n",
    "        \"r\": 16,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "    \"values2\": {\n",
    "        \"r\": 64,\n",
    "        \"lora_alpha\": 128,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config1 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    r=peft_values[\"values1\"][\"r\"],\n",
    "    lora_alpha=peft_values[\"values1\"][\"lora_alpha\"],\n",
    "    lora_dropout=peft_values[\"values1\"][\"lora_dropout\"],\n",
    "    bias=peft_values[\"values1\"][\"bias\"],\n",
    "    target_modules=[\"query\", \"value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e13bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "lora_model1 = get_peft_model(model, lora_config1)\n",
    "lora_model1.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3e398-5fd0-41c1-ae5c-44f311a84d9d",
   "metadata": {},
   "source": [
    "### Train the PEFT model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46862f9-750f-405e-af9e-728051e2b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_peft1 = TrainingArguments(\n",
    "    \"trainer_peft1_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer1 = Trainer(\n",
    "    model=lora_model1,\n",
    "    args=training_args_peft1,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer1.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9af57",
   "metadata": {},
   "source": [
    "###### **With fine tuning the model1 \"google-bert/bert-base-cased\" the _accuracy_ is now _0.849_ much better than the performance of the original foundational model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b2070",
   "metadata": {},
   "source": [
    "### Save the PEFT model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model1.save_pretrained(\"trainer_peft_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltra trainer_peft_1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc0b88-6ae6-453f-b8ab-75e8d0ead40c",
   "metadata": {},
   "source": [
    "### Create PEFT model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a302c8ec-c10d-430d-89a2-7e7a917917ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config2 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    r=peft_values[\"values2\"][\"r\"],\n",
    "    lora_alpha=peft_values[\"values2\"][\"lora_alpha\"],\n",
    "    lora_dropout=peft_values[\"values2\"][\"lora_dropout\"],\n",
    "    bias=peft_values[\"values2\"][\"bias\"],\n",
    "    target_modules=[\"query\", \"value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23842433-d641-4c91-adcc-048bbb88bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "lora_model2 = get_peft_model(model, lora_config2)\n",
    "lora_model2.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc41b7-1d61-4d88-a2f9-ba536b3f1900",
   "metadata": {},
   "source": [
    "### Train PEFT model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85654d-f9a8-49e7-8bd0-f402e2ab9078",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_peft2 = TrainingArguments(\n",
    "    \"trainer_peft2_output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c5076-a39c-4dd1-b4ff-329c9d57f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer2 = Trainer(\n",
    "    model=lora_model2,\n",
    "    args=training_args_peft2,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c837e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf6c6f",
   "metadata": {},
   "source": [
    "**With fine tuning the model2 \"google-bert/bert-base-cased\" the _accuracy_ is now _0.877_ much better than the performance of the original foundational model and the PEFT1 model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2447c43",
   "metadata": {},
   "source": [
    "### Save the PEFT model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model1.save_pretrained(\"trainer_peft_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01468279",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltra trainer_peft_2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefb3fe-6110-49f4-981c-7a87cb76b2e1",
   "metadata": {},
   "source": [
    "## Perform Inference Using the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679d481",
   "metadata": {},
   "source": [
    "### Load the saved PEFT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d6d180",
   "metadata": {},
   "source": [
    "We load the best PEFT model of the two we created: \"trainer_peft_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = AutoModelForSequenceClassification.from_pretrained(\"trainer_peft_2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c08ece",
   "metadata": {},
   "source": [
    "### Evaluate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# classify function from URL: https://achimoraites.medium.com/lightweight-roberta-sequence-classification-fine-tuning-with-lora-using-the-hugging-face-peft-8dd9edf99d19\n",
    "\n",
    "x = random.randint(0, 1000)\n",
    "\n",
    "text_to_classify=small_eval_dataset[\"text\"][x]\n",
    "\n",
    "def classify(text):\n",
    "  inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "  output = saved_model(**inputs)\n",
    "\n",
    "  prediction = output.logits.argmax(dim=-1).item()\n",
    "\n",
    "  print(f'\\n Class: {prediction}, Label: {id2label[prediction]},\\nText: {text}')\n",
    "\n",
    "\n",
    "\n",
    "classify(text_to_classify)\n",
    "\n",
    "print(\"\\nFrom the dataset, the text is classified as: {}: {}\\n\".format(small_eval_dataset[\"label\"][x], id2label[small_eval_dataset[\"label\"][x]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03bc9e",
   "metadata": {},
   "source": [
    "**The inference classified the text as negative which matches what the datatsets has as label.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
