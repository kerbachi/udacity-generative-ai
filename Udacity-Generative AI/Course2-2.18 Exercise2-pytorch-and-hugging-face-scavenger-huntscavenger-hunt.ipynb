{"cells":[{"cell_type":"markdown","metadata":{"id":"eE_kCT1Mb6VZ"},"source":["# Exercise: PyTorch and HuggingFace scavenger hunt!\n","\n","PyTorch and HuggingFace have emerged as powerful tools for developing and deploying neural networks.\n","\n","In this scavenger hunt, we will explore the capabilities of PyTorch and HuggingFace, uncovering hidden treasures on the way.\n","\n","We have two parts:\n","* Familiarize yourself with PyTorch\n","* Get to know HuggingFace"]},{"cell_type":"markdown","metadata":{"id":"-GtSoE-lb6Vb"},"source":["## Familiarize yourself with PyTorch\n","\n","Learn the basics of PyTorch, including tensors, neural net parts, loss functions, and optimizers. This will provide a foundation for understanding and utilizing its capabilities in developing and training neural networks."]},{"cell_type":"markdown","metadata":{"id":"MFLRCefZb6Vb"},"source":["### PyTorch tensors\n","\n","Scan through the PyTorch tensors documentation [here](https://pytorch.org/docs/stable/tensors.html). Be sure to look at the examples.\n","\n","In the following cell, create a tensor named `my_tensor` of size 3x3 with values of your choice. The tensor should be created on the GPU if available. Print the tensor."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yFrMCEZwb6Vc","executionInfo":{"status":"ok","timestamp":1726013799288,"user_tz":240,"elapsed":8327,"user":{"displayName":"Mohamed Kerbachi","userId":"14526026949866334344"}},"outputId":"6a4a37d7-b372-400b-bd8a-f3fb882241d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","tensor([[ 1., -1.,  2.],\n","        [ 1., -1.,  2.],\n","        [ 1., -1.,  2.]])\n"]}],"source":["# Fill in the missing parts labelled <MASK> with the appropriate code to complete the exercise.\n","\n","# Hint: Use torch.cuda.is_available() to check if GPU is available\n","\n","import torch\n","\n","print(torch.cuda.is_available())\n","\n","# Set the device to be used for the tensor\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Create a tensor on the appropriate device\n","# my_tensor = <MASK>\n","my_tensor = torch.tensor([[1., -1., 2], [1., -1., 2], [1., -1.,2 ]])\n","\n","# Print the tensor\n","print(my_tensor)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"wShvoU3ob6Vc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726013801495,"user_tz":240,"elapsed":226,"user":{"displayName":"Mohamed Kerbachi","userId":"14526026949866334344"}},"outputId":"949e0b3d-89c2-49c8-bdc7-082486680f8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Success!\n"]}],"source":["# Check the previous cell\n","\n","assert my_tensor.device.type in {\"cuda\", \"cpu\"}\n","assert my_tensor.shape == (3, 3)\n","\n","print(\"Success!\")"]},{"cell_type":"markdown","metadata":{"id":"4-35PAyXb6Vd"},"source":["### Neural Net Constructor Kit `torch.nn`\n","\n","You can think of the `torch.nn` ([documentation](https://pytorch.org/docs/stable/nn.html)) module as a constructor kit for neural networks. It provides the building blocks for creating neural networks, including layers, activation functions, loss functions, and more.\n","\n","Instructions:\n","\n","Create a three layer Multi-Layer Perceptron (MLP) neural network with the following specifications:\n","\n","- Input layer: 784 neurons\n","- Hidden layer: 128 neurons\n","- Output layer: 10 neurons\n","\n","Use the ReLU activation function for the hidden layer and the softmax activation function for the output layer. Print the neural network.\n","\n","Hint: MLP's use \"fully-connected\" or \"dense\" layers. In PyTorch's `nn` module, this type of layer has a different name. See the examples in [this tutorial](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html) to find out more."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"GlYHbmvIb6Vd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726013806887,"user_tz":240,"elapsed":256,"user":{"displayName":"Mohamed Kerbachi","userId":"14526026949866334344"}},"outputId":"6ce1ae6f-5656-49fd-affb-7305a35e71b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["MyMLP(\n","  (fc1): Linear(in_features=784, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=10, bias=True)\n","  (softmax): Softmax(dim=1)\n",")\n"]}],"source":["# Replace <MASK> with the appropriate code to complete the exercise.\n","\n","import torch.nn as nn\n","\n","\n","class MyMLP(nn.Module):\n","    \"\"\"My Multilayer Perceptron (MLP)\n","\n","    Specifications:\n","\n","        - Input layer: 784 neurons\n","        - Hidden layer: 128 neurons with ReLU activation\n","        - Output layer: 10 neurons with softmax activation\n","\n","    \"\"\"\n","\n","    # def __init__(self):\n","    #     super(MyMLP, self).__init__()\n","    #     self.fc1 = <MASK>\n","    #     self.fc2 = <MASK>\n","    #     self.relu = <MASK>\n","    #     self.softmax = <MASK>\n","\n","    # def forward(self, x):\n","    #     # Pass the input to the second layer\n","    #     x = <MASK>\n","\n","    #     # Apply ReLU activation\n","    #     x = <MASK>\n","\n","    #     # Pass the result to the final layer\n","    #     x = <MASK>\n","\n","    #     # Apply softmax activation\n","    #     x = <MASK>\n","\n","    #     return x\n","\n","\n","    #Example and doc: https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html\n","    def __init__(self):\n","        super(MyMLP, self).__init__()\n","        self.fc1 = nn.Linear(784, 128) # First fully connected layer\n","        self.fc2 = nn.Linear(128, 10) # Second fully connected layer that outputs our 10 labels\n","        self.relu = nn.ReLU # https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n","        self.softmax = nn.Softmax(dim=1) # https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n","\n","    def forward(self, x):\n","        # Pass the input to the second layer\n","        x = self.fc1(x)\n","\n","        # Apply ReLU activation\n","        x = self.relu(x)\n","\n","        # Pass the result to the final layer\n","        x = self.fc2(x)\n","\n","        # Apply softmax activation\n","        x = self.softmax(x)\n","\n","        return x\n","\n","\n","my_mlp = MyMLP()\n","print(my_mlp)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"gdM8gRmDb6Ve","executionInfo":{"status":"ok","timestamp":1726013812489,"user_tz":240,"elapsed":207,"user":{"displayName":"Mohamed Kerbachi","userId":"14526026949866334344"}}},"outputs":[],"source":["# Check your work here:\n","\n","\n","# Check the number of inputs\n","assert my_mlp.fc1.in_features == 784\n","\n","# Check the number of outputs\n","assert my_mlp.fc2.out_features == 10\n","\n","# Check the number of nodes in the hidden layer\n","assert my_mlp.fc1.out_features == 128\n","\n","# Check that my_mlp.fc1 is a fully connected layer\n","assert isinstance(my_mlp.fc1, nn.Linear)\n","\n","# Check that my_mlp.fc2 is a fully connected layer\n","assert isinstance(my_mlp.fc2, nn.Linear)"]},{"cell_type":"markdown","metadata":{"id":"hDmthdIPb6Ve"},"source":["### PyTorch Loss Functions and Optimizers\n","\n","PyTorch comes with a number of built-in loss functions and optimizers that can be used to train neural networks. The loss functions are implemented in the `torch.nn` ([documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)) module, while the optimizers are implemented in the `torch.optim` ([documentation](https://pytorch.org/docs/stable/optim.html)) module.\n","\n","\n","Instructions:\n","\n","- Create a loss function using the `torch.nn.CrossEntropyLoss` ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)) class.\n","- Create an optimizer using the `torch.optim.SGD` ([documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)) class with a learning rate of 0.01.\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"3-jGgk35b6Ve","executionInfo":{"status":"ok","timestamp":1726015491588,"user_tz":240,"elapsed":221,"user":{"displayName":"Mohamed Kerbachi","userId":"14526026949866334344"}}},"outputs":[],"source":["# Replace <MASK> with the appropriate code to complete the exercise.\n","\n","# Loss function\n","# loss_fn = <MASK>\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Optimizer (by convention we use the variable optimizer)\n","# optimizer = <MASK>\n","optimizer = torch.optim.SGD(my_mlp.parameters(), lr=0.01, momentum=0.9)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"0Pnayessb6Vf","executionInfo":{"status":"ok","timestamp":1726015493244,"user_tz":240,"elapsed":171,"user":{"displayName":"Mohamed Kerbachi","userId":"14526026949866334344"}}},"outputs":[],"source":["# Check\n","\n","assert isinstance(\n","    loss_fn, nn.CrossEntropyLoss\n","), \"loss_fn should be an instance of CrossEntropyLoss\"\n","assert isinstance(optimizer, torch.optim.SGD), \"optimizer should be an instance of SGD\"\n","assert optimizer.defaults[\"lr\"] == 0.01, \"learning rate should be 0.01\"\n","assert optimizer.param_groups[0][\"params\"] == list(\n","    my_mlp.parameters()\n","), \"optimizer should be passed the MLP parameters\""]},{"cell_type":"markdown","metadata":{"id":"4ASulmlmb6Vf"},"source":["### PyTorch Training Loops\n","\n","PyTorch makes writing a training loop easy!\n","\n","\n","Instructions:\n","\n","- Fill in the blanks!"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Z0ZGWpomb6Vf","colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"status":"error","timestamp":1726015755268,"user_tz":240,"elapsed":188,"user":{"displayName":"Mohamed Kerbachi","userId":"14526026949866334344"}},"outputId":"1b12f1cd-8565-4a2d-a509-615a2f51b0a8"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"linear(): argument 'input' (position 1) must be Tensor, not ReLU","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-816d5481345f>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Forward pass (predictions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# y_pred = <MASK>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Compute the loss and its gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-98d78d3ae22f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Pass the result to the final layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Apply softmax activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not ReLU"]}],"source":["# Replace <MASK> with the appropriate code to complete the exercise.\n","\n","def fake_training_loaders():\n","    for _ in range(30):\n","        yield torch.randn(64, 784), torch.randint(0, 10, (64,))\n","\n","\n","for epoch in range(3):\n","    # Create a training loop\n","    for i, data in enumerate(fake_training_loaders()):\n","        # Every data instance is an input + label pair\n","        x, y = data\n","\n","        # Zero your gradients for every batch!\n","        # <MASK>\n","        optimizer.zero_grad()\n","\n","        # Forward pass (predictions)\n","        # y_pred = <MASK>\n","        y_pred = my_mlp(x)\n","\n","        # Compute the loss and its gradients\n","        # loss = <MASK>\n","        # <MASK>\n","        loss = loss_fn(y_pred, y)\n","        loss.backward()\n","\n","        # Adjust learning weights\n","        # <MASK>\n","        optimizer.step()\n","\n","        if i % 10 == 0:\n","            print(f\"Epoch {epoch}, batch {i}: {loss.item():.5f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EAfWGcm5b6Vf"},"outputs":[],"source":["# Check\n","\n","assert abs(loss.item() - 2.3) < 0.1, \"the loss should be around 2.3 with random data\""]},{"cell_type":"markdown","metadata":{"id":"WGWBysRob6Vf"},"source":["Great job! Now you know the basics of PyTorch! Let's turn to HuggingFace 🤗."]},{"cell_type":"markdown","metadata":{"id":"NB9cHzxmb6Vg"},"source":["## Get to know HuggingFace\n","\n","HuggingFace is a popular destination for pre-trained models and datasets that can be applied to a variety of tasks quickly and easily. In this section, we will explore the capabilities of HuggingFace and learn how to use it to build and train neural networks."]},{"cell_type":"markdown","metadata":{"id":"1dEOQ-D9b6Vg"},"source":["### Download a model from HuggingFace and use it for sentiment analysis\n","\n","HuggingFace provides a number of pre-trained models that can be used for a variety of tasks. In this exercise, we will use the `distilbert-base-uncased-finetuned-sst-2-english` model to perform sentiment analysis on a movie review.\n","\n","Instructions:\n","- Review the [AutoModel tutorial](https://huggingface.co/docs/transformers/quicktour#automodel) on the HuggingFace website.\n","- Instantiate an AutoModelForSequenceClassification model using the `distilbert-base-uncased-finetuned-sst-2-english` model.\n","- Instantiate an AutoTokenizer using the `distilbert-base-uncased-finetuned-sst-2-english` model.\n","- Define a function that will get a prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NYASuXftb6Vg"},"outputs":[],"source":["# Replace <MASK> with the appropriate code to complete the exercise.\n","\n","# Get the model and tokenizer\n","\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","pt_model = <MASK>\n","tokenizer = <MASK>\n","\n","\n","def get_prediction(review):\n","    \"\"\"Given a review, return the predicted sentiment\"\"\"\n","\n","    # Tokenize the review\n","    # (Get the response as tensors and not as a list)\n","    inputs = <MASK>\n","\n","    # Perform the prediction (get the logits)\n","    outputs = pt_model(**inputs)\n","\n","    # Get the predicted class (corresponding to the highest logit)\n","    predictions = torch.argmax(outputs.logits, dim=-1)\n","\n","    return \"positive\" if predictions.item() == 1 else \"negative\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScsyIDPib6Vg"},"outputs":[],"source":["# Check\n","\n","review = \"This movie is not so great :(\"\n","\n","print(f\"Review: {review}\")\n","print(f\"Sentiment: {get_prediction(review)}\")\n","\n","assert get_prediction(review) == \"negative\", \"The prediction should be negative\"\n","\n","\n","review = \"This movie rocks!\"\n","\n","print(f\"Review: {review}\")\n","print(f\"Sentiment: {get_prediction(review)}\")\n","\n","assert get_prediction(review) == \"positive\", \"The prediction should be positive\""]},{"cell_type":"markdown","metadata":{"id":"BitPfJcxb6Vg"},"source":["### Download a dataset from HuggingFace\n","\n","HuggingFace provides a number of datasets that can be used for a variety of tasks. In this exercise, we will use the `imdb` dataset and pass it to the model we instantiated in the previous exercise.\n","\n","Instructions:\n","- Review the [loading a dataset](https://huggingface.co/docs/datasets/v1.11.0/loading_datasets.html) documentation\n","- Fill in the blanks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDttfY_Tb6Vg"},"outputs":[],"source":["# Replace <MASK> with the appropriate code\n","\n","from datasets import load_dataset\n","\n","# Load the test split of the imdb dataset\n","dataset = <MASK>\n","\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mF3qvrBFb6Vg"},"outputs":[],"source":["# Check\n","\n","from pprint import pprint\n","\n","from datasets import Dataset\n","\n","assert isinstance(dataset, Dataset), \"The dataset should be a Dataset object\"\n","assert set(dataset.features.keys()) == {\n","    \"label\",\n","    \"text\",\n","}, \"The dataset should have a label and a text feature\"\n","\n","# Show the first example\n","pprint(dataset[0])"]},{"cell_type":"markdown","metadata":{"id":"QC_JXBKvb6Vh"},"source":["### Now let's use the pre-trained model!\n","\n","Let's make some predictions.\n","\n","Instructions:\n","- Fill in the blanks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWI61owGb6Vh"},"outputs":[],"source":["# Replace <MASK> with the appropriate code\n","\n","# Get the last 3 reviews\n","reviews = dataset[\"text\"][-3:]\n","\n","# Get the last 3 labels\n","labels = dataset[\"label\"][-3:]\n","\n","# Check\n","for review, label in zip(reviews, labels):\n","    # Let's use your get_prediction function to get the sentiment\n","    # of the review!\n","    prediction = <MASK>\n","\n","    print(f\"Review: {review[:80]} \\n... {review[-80:]}\")\n","    print(f'Label: {\"positive\" if label else \"negative\"}')\n","    print(f\"Prediction: {prediction}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"SMNTg4Jcb6Vh"},"source":["Congrats for finishing the exercise! 🎉🎉🎉"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}